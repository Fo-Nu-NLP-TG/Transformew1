{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11360328,"sourceType":"datasetVersion","datasetId":7110175}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clone a GitHub repository in Kaggle\n!git clone https://github.com/Lemniscate-world/FoNu_NLP_TG.git\n\n# Navigate to the cloned repository\n%cd FoNu_NLP_TG\n\n# List files to verify the clone worked\n!ls -la","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T23:36:06.517966Z","iopub.execute_input":"2025-04-10T23:36:06.518189Z","iopub.status.idle":"2025-04-10T23:36:15.390774Z","shell.execute_reply.started":"2025-04-10T23:36:06.518169Z","shell.execute_reply":"2025-04-10T23:36:15.389869Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'FoNu_NLP_TG'...\nremote: Enumerating objects: 375, done.\u001b[K\nremote: Counting objects: 100% (118/118), done.\u001b[K\nremote: Compressing objects: 100% (78/78), done.\u001b[K\nremote: Total 375 (delta 58), reused 92 (delta 39), pack-reused 257 (from 1)\u001b[K\nReceiving objects: 100% (375/375), 99.72 MiB | 25.64 MiB/s, done.\nResolving deltas: 100% (146/146), done.\nUpdating files: 100% (104/104), done.\nEncountered 1 file(s) that should have been pointers, but weren't:\n\tdata/processed/clean_ewe_english.csv\n/kaggle/working/FoNu_NLP_TG\ntotal 212\ndrwxr-xr-x 22 root root  4096 Apr 10 23:36  .\ndrwxr-xr-x  4 root root  4096 Apr 10 23:36  ..\ndrwxr-xr-x  4 root root  4096 Apr 10 23:36  Attention_Is_All_You_Need\ndrwxr-xr-x  3 root root  4096 Apr 10 23:36  blog\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  Colab\n-rw-r--r--  1 root root 24346 Apr 10 23:36  cours_fonu_nlp_tg.md\ndrwxr-xr-x  6 root root  4096 Apr 10 23:36  data\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  data_processing\n-rw-r--r--  1 root root   180 Apr 10 23:36  DATASETS.md\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  docs\n-rw-r--r--  1 root root 25186 Apr 10 23:36  ewe_english_translation_approaches.md\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  Explain_training\ndrwxr-xr-x  9 root root  4096 Apr 10 23:36  .git\n-rw-r--r--  1 root root   759 Apr 10 23:36  .gitattributes\ndrwxr-xr-x  3 root root  4096 Apr 10 23:36  .github\n-rw-r--r--  1 root root  1940 Apr 10 23:36  .gitignore\n-rw-r--r--  1 root root 12039 Apr 10 23:36  image.png\n-rw-r--r--  1 root root  3222 Apr 10 23:36  IMPORT_TROUBLESHOOTING.md\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  Kaggle\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  Kaggle_dataset\n-rw-r--r--  1 root root  3978 Apr 10 23:36  LARGE_FILES_HANDLING.md\n-rw-r--r--  1 root root    66 Apr 10 23:36  large-files.txt\n-rw-r--r--  1 root root  1067 Apr 10 23:36  LICENSE\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  Mini-Courses\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36 'Model&Training_Visualizations'\n-rw-r--r--  1 root root  9764 Apr 10 23:36  PROJECT_STRUCTURE.md\n-rw-r--r--  1 root root  6521 Apr 10 23:36  README.md\n-rwxr-xr-x  1 root root  2227 Apr 10 23:36  remove_large_files.sh\n-rw-r--r--  1 root root   413 Apr 10 23:36  requirements.txt\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  ResNets\ndrwxr-xr-x  3 root root  4096 Apr 10 23:36 'SV(Semantics_Visualizer)'\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  Tensorflow_Ytb_Channel\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  training_fixes\n-rwxr-xr-x  1 root root   601 Apr 10 23:36  train_transformer.py\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  Transfer_Learning\ndrwxr-xr-x  2 root root  4096 Apr 10 23:36  .vscode\ndrwxr-xr-x  4 root root  4096 Apr 10 23:36  Word2Vec\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 1. Install required dependencies\n!pip install sentencepiece transformers torch\n\n# 2. Navigate to your cloned repository\n%cd FoNu_NLP_TG\n\n# 3. Check GPU availability (Kaggle provides Tesla P100 or T4 GPUs)\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# 4. Prepare your data directory\n!mkdir -p data/processed\n\n# 5. Download and prepare your dataset\n# If using Kaggle dataset:\n!kaggle datasets download -d tchaye59/eweenglish-bilingual-pairs\n!unzip eweenglish-bilingual-pairs.zip -d data/raw\n!python data_processing/prepare_data.py --input_dir data/raw --output_dir data/processed\n\n# 6. Train tokenizers\n!python data_processing/run_tokenizer_training.py --data_dir ./data/processed --method sentencepiece\n\n# 7. Run the transformer training\n!python Attention_Is_All_You_Need/train_transformer.py ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T23:36:15.391905Z","iopub.execute_input":"2025-04-10T23:36:15.392192Z","iopub.status.idle":"2025-04-11T00:05:57.771839Z","shell.execute_reply.started":"2025-04-10T23:36:15.392162Z","shell.execute_reply":"2025-04-11T00:05:57.770958Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n[Errno 2] No such file or directory: 'FoNu_NLP_TG'\n/kaggle/working/FoNu_NLP_TG\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\nMemory: 17.06 GB\nTraceback (most recent call last):\n  File \"/usr/local/bin/kaggle\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n    out = args.func(**command_args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1734, in dataset_download_cli\n    with self.build_kaggle_client() as kaggle:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n    username=self.config_values['username'],\n             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'username'\nunzip:  cannot find or open eweenglish-bilingual-pairs.zip, eweenglish-bilingual-pairs.zip.zip or eweenglish-bilingual-pairs.zip.ZIP.\npython3: can't open file '/kaggle/working/FoNu_NLP_TG/data_processing/prepare_data.py': [Errno 2] No such file or directory\nFound corpus for ewe\nFound corpus for english\nFound corpus for french\n\nTraining SentencePiece tokenizers...\n\nTraining SentencePiece tokenizer for ewe...\nTraining SentencePiece tokenizer for ewe with unigram model\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: ./data/processed/ewe_corpus.txt\n  input_format: \n  model_prefix: ./data/processed/ewe_sp\n  model_type: UNIGRAM\n  vocab_size: 8000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 1000000\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  user_defined_symbols: <PAD>\n  user_defined_symbols: <UNK>\n  user_defined_symbols: <BOS>\n  user_defined_symbols: <EOS>\n  user_defined_symbols: <MASK>\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 1\n  bos_id: 2\n  eos_id: 3\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: ./data/processed/ewe_corpus.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 27149 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <UNK>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <BOS>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <EOS>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <MASK>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=3189245\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=183\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 27148 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1754022\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 56777 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 27148\ntrainer_interface.cc(609) LOG(INFO) Done! 42050\nunigram_model_trainer.cc(602) LOG(INFO) Using 42050 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=23003 obj=10.134 num_tokens=84019 num_tokens/piece=3.65252\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19146 obj=8.35172 num_tokens=84632 num_tokens/piece=4.42035\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14346 obj=8.30575 num_tokens=88996 num_tokens/piece=6.20354\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14329 obj=8.27764 num_tokens=89102 num_tokens/piece=6.2183\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10746 obj=8.36048 num_tokens=96182 num_tokens/piece=8.95049\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10743 obj=8.33584 num_tokens=96199 num_tokens/piece=8.95458\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=8.40476 num_tokens=101466 num_tokens/piece=11.5302\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=8.38354 num_tokens=101471 num_tokens/piece=11.5308\ntrainer_interface.cc(687) LOG(INFO) Saving model: ./data/processed/ewe_sp.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: ./data/processed/ewe_sp.vocab\nTrained ewe tokenizer with vocabulary size 8000\nTest tokenization for 'Ŋdi nyuie, èfɔ̀ nyuiê':\n['▁Ŋ', 'di', '▁nyuie', ',', '▁è', 'fɔ', '̀', '▁nyui', 'ê']\n\nTraining SentencePiece tokenizer for english...\nTraining SentencePiece tokenizer for english with unigram model\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: ./data/processed/english_corpus.txt\n  input_format: \n  model_prefix: ./data/processed/english_sp\n  model_type: UNIGRAM\n  vocab_size: 8000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 1000000\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  user_defined_symbols: <PAD>\n  user_defined_symbols: <UNK>\n  user_defined_symbols: <BOS>\n  user_defined_symbols: <EOS>\n  user_defined_symbols: <MASK>\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 1\n  bos_id: 2\n  eos_id: 3\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: ./data/processed/english_corpus.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 27149 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <UNK>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <BOS>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <EOS>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <MASK>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=3342687\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=155\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 27149 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1794770\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 68781 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 27149\ntrainer_interface.cc(609) LOG(INFO) Done! 46532\nunigram_model_trainer.cc(602) LOG(INFO) Using 46532 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25255 obj=10.2603 num_tokens=95087 num_tokens/piece=3.76508\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21482 obj=8.18502 num_tokens=95490 num_tokens/piece=4.44512\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16102 obj=8.1736 num_tokens=101833 num_tokens/piece=6.32425\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16096 obj=8.1522 num_tokens=101903 num_tokens/piece=6.33095\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12072 obj=8.28157 num_tokens=111908 num_tokens/piece=9.27005\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12072 obj=8.25277 num_tokens=111920 num_tokens/piece=9.27104\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9054 obj=8.43353 num_tokens=123523 num_tokens/piece=13.6429\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9054 obj=8.39797 num_tokens=123532 num_tokens/piece=13.6439\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=8.41495 num_tokens=124682 num_tokens/piece=14.1684\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=8.412 num_tokens=124685 num_tokens/piece=14.1687\ntrainer_interface.cc(687) LOG(INFO) Saving model: ./data/processed/english_sp.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: ./data/processed/english_sp.vocab\nTrained english tokenizer with vocabulary size 8000\nTest tokenization for 'Good morning, how are you?':\n['▁Good', '▁morning', ',', '▁how', '▁are', '▁you', '?']\n\nTraining SentencePiece tokenizer for french...\nTraining SentencePiece tokenizer for french with unigram model\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: ./data/processed/french_corpus.txt\n  input_format: \n  model_prefix: ./data/processed/french_sp\n  model_type: UNIGRAM\n  vocab_size: 8000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 1000000\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  user_defined_symbols: <PAD>\n  user_defined_symbols: <UNK>\n  user_defined_symbols: <BOS>\n  user_defined_symbols: <EOS>\n  user_defined_symbols: <MASK>\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 1\n  bos_id: 2\n  eos_id: 3\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: ./data/processed/french_corpus.txt\ntrainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\ntrainer_interface.cc(411) LOG(INFO) Sampled 1000000 sentences from 1060931 sentences.\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <UNK>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <BOS>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <EOS>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <MASK>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=53235694\ntrainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=847\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 999907 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=28671183\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 451344 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 999907\ntrainer_interface.cc(609) LOG(INFO) Done! 439602\nunigram_model_trainer.cc(602) LOG(INFO) Using 439602 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=185048 obj=12.2069 num_tokens=1037728 num_tokens/piece=5.60789\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=162524 obj=9.6647 num_tokens=1043262 num_tokens/piece=6.41913\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=121888 obj=9.64189 num_tokens=1083134 num_tokens/piece=8.88631\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=121837 obj=9.63079 num_tokens=1085517 num_tokens/piece=8.90958\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=91376 obj=9.68337 num_tokens=1143102 num_tokens/piece=12.5099\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=91374 obj=9.67158 num_tokens=1143089 num_tokens/piece=12.51\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=68530 obj=9.74505 num_tokens=1206000 num_tokens/piece=17.5981\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=68529 obj=9.73065 num_tokens=1205885 num_tokens/piece=17.5967\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=51396 obj=9.82346 num_tokens=1270896 num_tokens/piece=24.7275\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=51396 obj=9.80644 num_tokens=1271012 num_tokens/piece=24.7298\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38547 obj=9.92046 num_tokens=1336240 num_tokens/piece=34.6652\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=38547 obj=9.89992 num_tokens=1336152 num_tokens/piece=34.6629\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=28910 obj=10.0379 num_tokens=1402611 num_tokens/piece=48.5165\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=28910 obj=10.0126 num_tokens=1402608 num_tokens/piece=48.5164\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21682 obj=10.175 num_tokens=1469384 num_tokens/piece=67.7698\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21682 obj=10.1444 num_tokens=1469420 num_tokens/piece=67.7714\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16261 obj=10.3341 num_tokens=1536220 num_tokens/piece=94.4727\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16261 obj=10.2976 num_tokens=1536320 num_tokens/piece=94.4788\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12195 obj=10.5207 num_tokens=1609168 num_tokens/piece=131.953\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12195 obj=10.4778 num_tokens=1609205 num_tokens/piece=131.956\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9146 obj=10.7395 num_tokens=1684841 num_tokens/piece=184.216\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9146 obj=10.6868 num_tokens=1685528 num_tokens/piece=184.291\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=10.7204 num_tokens=1696762 num_tokens/piece=192.814\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=10.7133 num_tokens=1698099 num_tokens/piece=192.966\ntrainer_interface.cc(687) LOG(INFO) Saving model: ./data/processed/french_sp.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: ./data/processed/french_sp.vocab\nTrained french tokenizer with vocabulary size 8000\nTest tokenization for 'Bonjour, comment allez-vous?':\n['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?']\n\nTokenizer training complete!\nSentencePiece models are saved in: ./data/processed\nVisualization saved to mask_visualization.html\nalt.Chart(...)\nUsing device: cuda\nLoaded SentencePiece tokenizer with vocabulary size 8000\nLoaded SentencePiece tokenizer with vocabulary size 8000\nCreated dataset with 21719 translation pairs\nCreated dataset with 2715 translation pairs\nSource vocabulary size: 8000\nTarget vocabulary size: 8000\nCreating model with src_vocab_size=8000, tgt_vocab_size=8000\nGenerator output dimension: 8000\nEpoch 1/10\nOutput shape: torch.Size([32, 127, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 127])\nTarget flat shape: torch.Size([4064])\nBatch 0: Max target index: 7964, Vocab size: 8000\nTrain Loss: 5.8764\nSaved checkpoint to ./models/transformer_ewe_english_epoch1.pt\nEpoch 2/10\nOutput shape: torch.Size([32, 90, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 90])\nTarget flat shape: torch.Size([2880])\nBatch 0: Max target index: 7442, Vocab size: 8000\nTrain Loss: 4.9795\nSaved checkpoint to ./models/transformer_ewe_english_epoch2.pt\nEpoch 3/10\nOutput shape: torch.Size([32, 116, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 116])\nTarget flat shape: torch.Size([3712])\nBatch 0: Max target index: 7811, Vocab size: 8000\nTrain Loss: 4.6502\nSaved checkpoint to ./models/transformer_ewe_english_epoch3.pt\nEpoch 4/10\nOutput shape: torch.Size([32, 127, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 127])\nTarget flat shape: torch.Size([4064])\nBatch 0: Max target index: 7553, Vocab size: 8000\nTrain Loss: 4.4180\nSaved checkpoint to ./models/transformer_ewe_english_epoch4.pt\nEpoch 5/10\nOutput shape: torch.Size([32, 127, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 127])\nTarget flat shape: torch.Size([4064])\nBatch 0: Max target index: 7534, Vocab size: 8000\nTrain Loss: 4.2129\nSaved checkpoint to ./models/transformer_ewe_english_epoch5.pt\nEpoch 6/10\nOutput shape: torch.Size([32, 79, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 79])\nTarget flat shape: torch.Size([2528])\nBatch 0: Max target index: 7765, Vocab size: 8000\nTrain Loss: 4.0232\nSaved checkpoint to ./models/transformer_ewe_english_epoch6.pt\nEpoch 7/10\nOutput shape: torch.Size([32, 122, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 122])\nTarget flat shape: torch.Size([3904])\nBatch 0: Max target index: 7862, Vocab size: 8000\nTrain Loss: 3.8606\nSaved checkpoint to ./models/transformer_ewe_english_epoch7.pt\nEpoch 8/10\nOutput shape: torch.Size([32, 79, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 79])\nTarget flat shape: torch.Size([2528])\nBatch 0: Max target index: 7615, Vocab size: 8000\nTrain Loss: 3.7092\nSaved checkpoint to ./models/transformer_ewe_english_epoch8.pt\nEpoch 9/10\nOutput shape: torch.Size([32, 116, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 116])\nTarget flat shape: torch.Size([3712])\nBatch 0: Max target index: 7828, Vocab size: 8000\nTrain Loss: 3.5673\nSaved checkpoint to ./models/transformer_ewe_english_epoch9.pt\nEpoch 10/10\nOutput shape: torch.Size([32, 127, 8000]), Output size(-1): 8000\nTarget shape: torch.Size([32, 127])\nTarget flat shape: torch.Size([4064])\nBatch 0: Max target index: 7988, Vocab size: 8000\nTrain Loss: 3.4282\nSaved checkpoint to ./models/transformer_ewe_english_epoch10.pt\nSaved final model to ./models/transformer_ewe_english_final.pt\n","output_type":"stream"}],"execution_count":2}]}